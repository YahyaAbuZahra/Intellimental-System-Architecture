# -*- coding: utf-8 -*-
"""dataset_processing_nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4wIchuj8cfkDRitZe0rH6RRx9BZvEXh
"""

!pip install --upgrade pip
!pip install pandas scikit-learn tqdm

#!/usr/bin/env python3


import os
import sys
import json
import traceback
from datetime import datetime, timezone
from pathlib import Path

try:
    import pandas as pd
    import numpy as np
    import re
    from sklearn.model_selection import train_test_split
    from tqdm import tqdm
    from google.colab import files
except ImportError as e:
    print(" Missing required packages. Installing...")
    print("!pip install pandas scikit-learn tqdm")
    raise SystemExit(1)

LOG_PATH = Path("prepare_datasets.log")

def log(msg):
    """Write timestamped log message"""
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    line = f"[{timestamp}] {msg}"
    print(line)
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(line + "\n")

def upload_files_colab():
    """Upload files in Google Colab environment"""
    print(" Please upload your CSV files (at least 2 files)...")

    uploaded = files.upload()
    file_paths = []

    for filename, content in uploaded.items():
        with open(filename, 'wb') as f:
            f.write(content)
        file_paths.append(Path(filename))
        log(f" Uploaded: {filename} ({len(content):,} bytes)")

    return file_paths

def safe_read_csv(path):
    """Try multiple encodings to read CSV"""
    encodings = ["utf-8", "utf-8-sig", "latin1", "iso-8859-1", "cp1256"]

    for enc in encodings:
        try:
            df = pd.read_csv(path, encoding=enc)
            log(f" Loaded {path.name} with encoding '{enc}' | Shape: {df.shape}")
            return df
        except Exception as e:
            log(f" Failed encoding '{enc}': {str(e)[:100]}")

    raise ValueError(f"Could not read CSV file {path} with any encoding.")

def choose_text_label_columns(df, df_name="dataset"):
    """Interactive column selection with smart guessing"""
    print(f"\n{'='*60}")
    print(f" DATASET: {df_name}")
    print('='*60)
    print(f"Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
    print(f"\nAvailable columns: {list(df.columns)}")

    # Smart guessing
    text_keywords = ["text", "post", "utter", "sentence", "message", "content",
                     "transcript", "body", "statement", "comment", "tweet", "utterance"]
    label_keywords = ["label", "class", "genre", "category", "status", "tag",
                      "diagnosis", "type", "sentiment", "emotion", "target"]

    likely_texts = [c for c in df.columns if any(k in c.lower() for k in text_keywords)]
    likely_labels = [c for c in df.columns if any(k in c.lower() for k in label_keywords)]

    if likely_texts:
        print(f"\n Suggested TEXT columns: {likely_texts}")
    if likely_labels:
        print(f" Suggested LABEL columns: {likely_labels}")

    # Select text column
    while True:
        if likely_texts:
            text_col = input(f"\n Enter TEXT column name [press Enter for '{likely_texts[0]}']: ").strip()
            if text_col == "":
                text_col = likely_texts[0]
        else:
            text_col = input(f"\n Enter TEXT column name: ").strip()

        if text_col in df.columns:
            log(f"Selected TEXT column: {text_col}")
            break
        print(f" Column '{text_col}' not found. Try again.")

    # Select label column
    while True:
        if likely_labels:
            label_col = input(f"\n  Enter LABEL column name [Enter for '{likely_labels[0]}', or 'none' to skip]: ").strip()
            if label_col == "":
                label_col = likely_labels[0]
        else:
            label_col = input(f"\n  Enter LABEL column name [or 'none' to skip]: ").strip()

        if label_col.lower() == "none":
            label_col = None
            log("No label column selected")
            break
        if label_col in df.columns:
            log(f"Selected LABEL column: {label_col}")
            break
        print(f" Column '{label_col}' not found. Try again.")

    # Show sample
    print(f"\n Sample rows from {df_name}:")
    display_cols = [text_col]
    if label_col:
        display_cols.append(label_col)
    sample_df = df[display_cols].head(3)
    print(sample_df.to_string(index=False, max_colwidth=80))

    return text_col, label_col

def clean_text(s):
    """Clean and normalize text"""
    if pd.isna(s) or s is None:
        return ""

    s = str(s).strip()

    # Remove URLs
    s = re.sub(r'http\S+|www\.\S+', ' ', s)

    # Remove extra whitespace
    s = re.sub(r'\s+', ' ', s)

    # Remove special characters but keep unicode, punctuation
    s = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF.,!?;:\-\'\"]', ' ', s)

    # Final cleanup
    s = re.sub(r'\s+', ' ', s).strip()

    return s

def interactive_label_mapping(unique_labels, dataset_name):
    """Interactive label mapping with user guidance"""
    print(f"\n{'='*60}")
    print(f"  LABEL MAPPING FOR: {dataset_name}")
    print('='*60)
    print(f"Found {len(unique_labels)} unique labels:\n")

    for i, label in enumerate(unique_labels, 1):
        print(f"  [{i:2d}] {label}")

    print("\n" + "="*60)
    print(" INSTRUCTIONS:")
    print("  â€¢ Map each label to a UNIFIED name (e.g., 'Depression', 'Normal')")
    print("  â€¢ Type 'SKIP' to exclude rows with this label")
    print("  â€¢ Press Enter to keep the original label")
    print("="*60 + "\n")

    mapping = {}
    for label in unique_labels:
        while True:
            ans = input(f"Map '{label}' â†’ ").strip()

            if ans == "":
                ans = label
                print(f"  â†³ Keeping original: '{label}'")
            elif ans.upper() == "SKIP":
                print(f"  â†³ Will SKIP rows with label '{label}'")
            else:
                print(f"  â†³ Mapped to: '{ans}'")

            mapping[label] = ans
            break

    log(f"Label mapping for {dataset_name}: {mapping}")
    return mapping

def apply_label_mapping(df, mapping):
    """Apply label mapping and filter out SKIP labels"""
    df = df.copy()

    def map_label(label):
        if pd.isna(label):
            return None
        label_str = str(label).strip()
        mapped = mapping.get(label_str, label_str)
        return None if mapped.upper() == "SKIP" else mapped

    df['label_mapped'] = df['label'].apply(map_label)

    # Count mapping results
    total = len(df)
    kept = df['label_mapped'].notna().sum()
    skipped = total - kept

    log(f"Mapping results: {total:,} total â†’ {kept:,} kept, {skipped:,} skipped")

    return df[df['label_mapped'].notna()].copy()

def validate_labels(df, dataset_name):
    """Validate and clean label column"""
    log(f"Validating labels for {dataset_name}")

    # Convert to string and strip
    df['label'] = df['label'].astype(str).str.strip()

    # Remove any None, NaN, empty strings
    df = df[df['label'].notna()].copy()
    df = df[df['label'] != ''].copy()
    df = df[df['label'] != 'nan'].copy()
    df = df[df['label'] != 'None'].copy()

    # Check for valid labels
    unique_labels = df['label'].unique()
    log(f"Valid labels after cleanup: {len(unique_labels)} unique values")

    if len(unique_labels) < 2:
        raise ValueError(f"Need at least 2 different labels for stratification, found: {unique_labels}")

    return df

def main():
    log("="*60)
    log(" START DATASET PREPARATION PIPELINE")
    log("="*60)

    try:
        print("\n" + "="*60)
        print(" DATASET PREPARATION TOOL")
        print("="*60)
        print("This tool will:")
        print("  1. Merge multiple CSV datasets")
        print("  2. Unify and reduce label categories")
        print("  3. Clean text data")
        print("  4. Split into train/val/test sets")
        print("  5. Save prepared datasets")
        print("="*60 + "\n")

        # Upload files
        file_paths = upload_files_colab()

        if len(file_paths) < 2:
            raise ValueError(" Please upload at least 2 CSV files")

        p1, p2 = file_paths[0], file_paths[1]
        log(f"Working with: {p1.name} and {p2.name}")

        # Load datasets
        print(f"\n{'='*60}")
        print(" LOADING DATASETS")
        print('='*60)
        df1 = safe_read_csv(p1)
        df2 = safe_read_csv(p2)

        # Select columns
        t1, l1 = choose_text_label_columns(df1, df_name=p1.name)
        t2, l2 = choose_text_label_columns(df2, df_name=p2.name)

        if not l1 or not l2:
            raise ValueError(" Both datasets must have label columns for training")

        # Prepare datasets with only needed columns
        df1_prep = pd.DataFrame({
            'text': df1[t1],
            'label': df1[l1],
            'source': p1.name
        })

        df2_prep = pd.DataFrame({
            'text': df2[t2],
            'label': df2[l2],
            'source': p2.name
        })

        # Get unique labels
        unique1 = sorted(df1_prep['label'].dropna().astype(str).str.strip().unique())
        unique2 = sorted(df2_prep['label'].dropna().astype(str).str.strip().unique())

        print(f"\n{'='*60}")
        print(" LABEL STATISTICS")
        print('='*60)
        print(f"\n{p1.name}:")
        print(f"  Unique labels: {len(unique1)}")
        print(f"  Distribution:\n{df1_prep['label'].value_counts()}\n")

        print(f"{p2.name}:")
        print(f"  Unique labels: {len(unique2)}")
        print(f"  Distribution:\n{df2_prep['label'].value_counts()}")

        # Interactive label mapping
        map1 = interactive_label_mapping(unique1, p1.name)
        map2 = interactive_label_mapping(unique2, p2.name)

        # Apply mappings
        print(f"\n{'='*60}")
        print(" APPLYING LABEL MAPPINGS")
        print('='*60)

        df1_mapped = apply_label_mapping(df1_prep, map1)
        df1_mapped['label'] = df1_mapped['label_mapped']
        df1_mapped = df1_mapped.drop(columns=['label_mapped'])

        df2_mapped = apply_label_mapping(df2_prep, map2)
        df2_mapped['label'] = df2_mapped['label_mapped']
        df2_mapped = df2_mapped.drop(columns=['label_mapped'])

        # Validate labels before merging
        df1_mapped = validate_labels(df1_mapped, p1.name)
        df2_mapped = validate_labels(df2_mapped, p2.name)

        # Clean text
        print(f"\n{'='*60}")
        print("ðŸ§¹ CLEANING TEXT")
        print('='*60)

        tqdm.pandas(desc=f"Cleaning {p1.name}")
        df1_mapped['text'] = df1_mapped['text'].progress_apply(clean_text)

        tqdm.pandas(desc=f"Cleaning {p2.name}")
        df2_mapped['text'] = df2_mapped['text'].progress_apply(clean_text)

        # Remove empty/short texts
        df1_mapped = df1_mapped[df1_mapped['text'].str.len() > 5].copy()
        df2_mapped = df2_mapped[df2_mapped['text'].str.len() > 5].copy()

        log(f"After cleaning: {p1.name}={len(df1_mapped):,} rows, {p2.name}={len(df2_mapped):,} rows")

        # Merge datasets
        print(f"\n{'='*60}")
        print("ðŸ”— MERGING DATASETS")
        print('='*60)

        combined = pd.concat([df1_mapped, df2_mapped], ignore_index=True)
        log(f"Combined shape: {combined.shape}")

        # Validate combined labels
        combined = validate_labels(combined, "combined")

        # Shuffle
        combined = combined.sample(frac=1, random_state=42).reset_index(drop=True)

        # Show final distribution
        print(f"\n{'='*60}")
        print(" FINAL LABEL DISTRIBUTION")
        print('='*60)
        print(combined['label'].value_counts())
        print(f"\nTotal samples: {len(combined):,}")
        print(f"Unique labels: {combined['label'].nunique()}")

        # Check if we have enough samples per class for splitting
        min_samples = combined['label'].value_counts().min()
        if min_samples < 3:
            raise ValueError(f"Not enough samples for splitting. Minimum class has only {min_samples} samples (need at least 3)")

        # Split data
        print(f"\n{'='*60}")
        print("  SPLITTING DATA")
        print('='*60)
        print("Split ratios: 70% train, 15% validation, 15% test")

        # First split: 70% train, 30% temp
        train_df, temp_df = train_test_split(
            combined,
            test_size=0.30,
            stratify=combined['label'],
            random_state=42
        )

        # Second split: split temp into 50% val, 50% test
        val_df, test_df = train_test_split(
            temp_df,
            test_size=0.50,
            stratify=temp_df['label'],
            random_state=42
        )

        print(f"\n Split completed:")
        print(f"  Train:      {len(train_df):,} samples ({len(train_df)/len(combined)*100:.1f}%)")
        print(f"  Validation: {len(val_df):,} samples ({len(val_df)/len(combined)*100:.1f}%)")
        print(f"  Test:       {len(test_df):,} samples ({len(test_df)/len(combined)*100:.1f}%)")

        # Save outputs
        print(f"\n{'='*60}")
        print(" SAVING OUTPUTS")
        print('='*60)

        out_dir = Path("prepared_data")
        out_dir.mkdir(exist_ok=True)

        train_df.to_csv(out_dir / "train.csv", index=False, encoding="utf-8")
        val_df.to_csv(out_dir / "validation.csv", index=False, encoding="utf-8")
        test_df.to_csv(out_dir / "test.csv", index=False, encoding="utf-8")
        combined.to_csv(out_dir / "combined_final.csv", index=False, encoding="utf-8")

        # Save mapping documentation
        mapping_doc = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "dataset1": {
                "filename": str(p1.name),
                "original_shape": df1.shape,
                "text_column": t1,
                "label_column": l1,
                "mapping": map1
            },
            "dataset2": {
                "filename": str(p2.name),
                "original_shape": df2.shape,
                "text_column": t2,
                "label_column": l2,
                "mapping": map2
            },
            "final_statistics": {
                "total_samples": len(combined),
                "unique_labels": combined['label'].nunique(),
                "label_distribution": combined['label'].value_counts().to_dict(),
                "train_size": len(train_df),
                "validation_size": len(val_df),
                "test_size": len(test_df)
            }
        }

        with open(out_dir / "mapping_documentation.json", "w", encoding="utf-8") as f:
            json.dump(mapping_doc, f, indent=2, ensure_ascii=False)

        # Create markdown documentation
        with open(out_dir / "README.md", "w", encoding="utf-8") as f:
            f.write("# Dataset Preparation Documentation\n\n")
            f.write(f"**Generated:** {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n")
            f.write("## Source Datasets\n\n")
            f.write(f"### Dataset 1: {p1.name}\n")
            f.write(f"- Shape: {df1.shape}\n")
            f.write(f"- Text column: `{t1}`\n")
            f.write(f"- Label column: `{l1}`\n\n")
            f.write("**Label Mapping:**\n")
            for k, v in map1.items():
                f.write(f"- `{k}` â†’ `{v}`\n")
            f.write(f"\n### Dataset 2: {p2.name}\n")
            f.write(f"- Shape: {df2.shape}\n")
            f.write(f"- Text column: `{t2}`\n")
            f.write(f"- Label column: `{l2}`\n\n")
            f.write("**Label Mapping:**\n")
            for k, v in map2.items():
                f.write(f"- `{k}` â†’ `{v}`\n")
            f.write("\n## Final Dataset Statistics\n\n")
            f.write(f"- **Total samples:** {len(combined):,}\n")
            f.write(f"- **Unique labels:** {combined['label'].nunique()}\n")
            f.write(f"- **Train size:** {len(train_df):,} ({len(train_df)/len(combined)*100:.1f}%)\n")
            f.write(f"- **Validation size:** {len(val_df):,} ({len(val_df)/len(combined)*100:.1f}%)\n")
            f.write(f"- **Test size:** {len(test_df):,} ({len(test_df)/len(combined)*100:.1f}%)\n\n")
            f.write("### Label Distribution\n\n")
            f.write("| Label | Count | Percentage |\n")
            f.write("|-------|-------|------------|\n")
            for label, count in combined['label'].value_counts().items():
                pct = count / len(combined) * 100
                f.write(f"| {label} | {count:,} | {pct:.2f}% |\n")

        print(f"\n All files saved to: {out_dir.resolve()}")
        print("\n Generated files:")
        print("  â€¢ train.csv")
        print("  â€¢ validation.csv")
        print("  â€¢ test.csv")
        print("  â€¢ combined_final.csv")
        print("  â€¢ mapping_documentation.json")
        print("  â€¢ README.md")

        log("="*60)
        log(" DATASET PREPARATION COMPLETED SUCCESSFULLY")
        log("="*60)

    except Exception as e:
        log(f" ERROR: {str(e)}")
        print(f"\n An error occurred: {str(e)}")
        traceback.print_exc()

        error_file = Path("prepare_datasets_error.txt")
        with open(error_file, "w", encoding="utf-8") as ef:
            ef.write(f"Error: {str(e)}\n\n")
            ef.write(traceback.format_exc())

        log(f"Full error traceback saved to: {error_file}")
        sys.exit(1)

if __name__ == '__main__':
    main()

import shutil
from google.colab import files

shutil.make_archive('/content/prepared_data', 'zip', '/content/prepared_data')

print("ðŸ“¦ Downloading prepared_data.zip...")
files.download('/content/prepared_data.zip')