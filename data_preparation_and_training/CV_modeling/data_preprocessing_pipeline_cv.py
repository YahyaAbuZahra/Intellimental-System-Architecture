# -*- coding: utf-8 -*-
"""data_preprocessing_pipeline_CV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ICtQUcqXtcA6I__REhS7un5CF0ysiNxu
"""

# Install necessary libraries for image processing, face detection, and augmentation
!pip install mtcnn opencv-python pillow torch torchvision albumentations --quiet

from google.colab import drive

# Mount Google Drive to access the datasets
drive.mount('/content/drive')

import os
import glob
import cv2
import pandas as pd
from mtcnn import MTCNN
import albumentations as A
from tqdm import tqdm
import csv
import gc
import torch
import zipfile

# ===================== PATHS =====================
zip_path = "/content/drive/MyDrive/CV datasets.zip"
extract_path = "/content/CV_datasets"
processed_path = "/content/drive/MyDrive/processed_dataset"
metadata_file = os.path.join(processed_path, "metadata.csv")

os.makedirs(processed_path, exist_ok=True)

try:
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print("‚úì Dataset extracted successfully")
except Exception as e:
    print(f" Error extracting zip: {e}")
    raise

# ===================== INITIALIZE MTCNN =====================
detector = MTCNN()

# ===================== AUGMENTATION PIPELINE =====================
augmentation = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15, p=0.5),
    A.RandomBrightnessContrast(p=0.5),
    A.GaussNoise(p=0.3),
    A.ColorJitter(p=0.5),
    A.RandomGamma(p=0.3),
    A.Blur(blur_limit=3, p=0.3),
    A.CoarseDropout(max_holes=1, max_height=20, max_width=20, min_holes=1, min_height=10, min_width=10, p=0.2)
])

# ===================== HELPER FUNCTIONS =====================
def crop_faces(image_path, confidence_threshold=0.90):
    try:
        image = cv2.imread(image_path)
        if image is None:
            return []
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        faces_detected = detector.detect_faces(image_rgb)
        faces = []
        for face in faces_detected:
            if face['confidence'] < confidence_threshold:
                continue
            x, y, w, h = face['box']
            x, y = max(0, x), max(0, y)
            x_end = min(x + w, image_rgb.shape[1])
            y_end = min(y + h, image_rgb.shape[0])
            cropped = image_rgb[y:y_end, x:x_end]
            if cropped.size > 0:
                faces.append(cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR))
        return faces
    except:
        return []

def resize_image(image, size=(224,224)):
    return cv2.resize(image, size, interpolation=cv2.INTER_AREA)

def process_and_save(image_paths, label, dataset_name, save_dir, augment=True):
    os.makedirs(os.path.join(save_dir, label), exist_ok=True)
    metadata = []
    for img_path in image_paths:
        faces = crop_faces(img_path)
        for i, face in enumerate(faces):
            face_resized = resize_image(face)
            base_filename = os.path.splitext(os.path.basename(img_path))[0]
            ext = ".jpg"
            if len(faces) > 1:
                filename = f"{base_filename}_face{i}{ext}"
            else:
                filename = f"{base_filename}{ext}"
            out_path = os.path.join(save_dir, label, filename)
            cv2.imwrite(out_path, face_resized)
            metadata.append([img_path, out_path, label, dataset_name, "original"])
            # Augmentation
            if augment:
                aug_face = augmentation(image=face_resized)['image']
                aug_filename = f"aug_{filename}"
                aug_out_path = os.path.join(save_dir, label, aug_filename)
                cv2.imwrite(aug_out_path, aug_face)
                metadata.append([img_path, aug_out_path, label, dataset_name, "augmented"])
    return metadata

def save_metadata(metadata, file_path=metadata_file):
    if os.path.exists(file_path):
        df_old = pd.read_csv(file_path)
        df_new = pd.DataFrame(metadata, columns=["original_path", "processed_path", "label", "dataset_name", "augmentation_type"])
        df_combined = pd.concat([df_old, df_new], ignore_index=True)
        df_combined.to_csv(file_path, index=False)
    else:
        df_new = pd.DataFrame(metadata, columns=["original_path", "processed_path", "label", "dataset_name", "augmentation_type"])
        df_new.to_csv(file_path, index=False)

def clean_memory():
    gc.collect()
    torch.cuda.empty_cache()

fer_train = glob.glob(os.path.join(extract_path, "**/data_set/train"), recursive=True)
if fer_train:
    fer_train = fer_train[0]
else:
    fer_train = os.path.join(extract_path, "CV datasets/DB/train")

fer_images = glob.glob(os.path.join(fer_train, "**/*.jpg"), recursive=True) + \
             glob.glob(os.path.join(fer_train, "**/*.png"), recursive=True)



all_metadata = []

for emotion, label in fer_mapping.items():
    emotion_images = [img for img in fer_images if emotion in img.lower()]
    print(f"Processing {emotion} ‚Üí {label}: {len(emotion_images)} images")
    metadata = process_and_save(emotion_images, label, "DB", processed_path)
    all_metadata.extend(metadata)

save_metadata(all_metadata)
clean_memory()

print(f"‚úì DB processing complete. Metadata saved at {metadata_file}")

import os

processed_path = "/content/drive/MyDrive/processed_dataset"

for label in os.listdir(processed_path):
    label_path = os.path.join(processed_path, label)
    if os.path.isdir(label_path):
        num_files = len([f for f in os.listdir(label_path) if os.path.isfile(os.path.join(label_path, f))])
        print(f"{label}: {num_files} file")

import os
import glob
import cv2
import pandas as pd
from mtcnn import MTCNN
import albumentations as A
from tqdm import tqdm
import gc
import torch
import zipfile

# ===================== PATHS =====================
zip_path = "/content/drive/MyDrive/CV datasets.zip"
extract_path = "/content/CV_datasets"
processed_path = "/content/drive/MyDrive/processed_dataset"
metadata_file = os.path.join(processed_path, "metadata.csv")

os.makedirs(processed_path, exist_ok=True)

# ===================== UNZIP =====================
try:
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print(" Dataset extracted successfully")
except Exception as e:
    print(f" Error extracting zip: {e}")
    raise

# ===================== INITIALIZE MTCNN =====================
detector = MTCNN()

# ===================== AUGMENTATION PIPELINE =====================
augmentation = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15, p=0.5),
    A.RandomBrightnessContrast(p=0.5),
    A.GaussNoise(p=0.3),
    A.ColorJitter(p=0.5),
    A.RandomGamma(p=0.3),
    A.Blur(blur_limit=3, p=0.3),
    A.CoarseDropout(num_holes_range=(1, 1), hole_height_range=(10, 20), hole_width_range=(10, 20), p=0.2)
])

# ===================== HELPER FUNCTIONS =====================
def crop_faces(image_path, confidence_threshold=0.90):
    try:
        image = cv2.imread(image_path)
        if image is None:
            return []
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        faces_detected = detector.detect_faces(image_rgb)
        faces = []
        for face in faces_detected:
            if face['confidence'] < confidence_threshold:
                continue
            x, y, w, h = face['box']
            x, y = max(0, x), max(0, y)
            x_end = min(x + w, image_rgb.shape[1])
            y_end = min(y + h, image_rgb.shape[0])
            cropped = image_rgb[y:y_end, x:x_end]
            if cropped.size > 0:
                faces.append(cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR))
        return faces
    except:
        return []

def resize_image(image, size=(224,224)):
    return cv2.resize(image, size, interpolation=cv2.INTER_AREA)

def process_and_save(image_paths, label, dataset_name, save_dir, augment=True):
    os.makedirs(os.path.join(save_dir, label), exist_ok=True)
    metadata = []
    for img_path in tqdm(image_paths, desc=f"Processing {label}"):
        faces = crop_faces(img_path)
        for i, face in enumerate(faces):
            face_resized = resize_image(face)
            base_filename = os.path.splitext(os.path.basename(img_path))[0]
            ext = ".jpg"
            if len(faces) > 1:
                filename = f"{base_filename}_face{i}{ext}"
            else:
                filename = f"{base_filename}{ext}"
            out_path = os.path.join(save_dir, label, filename)
            cv2.imwrite(out_path, face_resized)
            metadata.append([img_path, out_path, label, dataset_name, "original"])

            if augment:
                aug_face = augmentation(image=face_resized)['image']
                aug_filename = f"aug_{filename}"
                aug_out_path = os.path.join(save_dir, label, aug_filename)
                cv2.imwrite(aug_out_path, aug_face)
                metadata.append([img_path, aug_out_path, label, dataset_name, "augmented"])
    return metadata

def save_metadata(metadata, file_path=metadata_file):
    if os.path.exists(file_path):
        df_old = pd.read_csv(file_path)
        df_new = pd.DataFrame(metadata, columns=["original_path", "processed_path", "label", "dataset_name", "augmentation_type"])
        df_combined = pd.concat([df_old, df_new], ignore_index=True)
        df_combined.to_csv(file_path, index=False)
    else:
        df_new = pd.DataFrame(metadata, columns=["original_path", "processed_path", "label", "dataset_name", "augmentation_type"])
        df_new.to_csv(file_path, index=False)

def clean_memory():
    gc.collect()
    torch.cuda.empty_cache()

print("\n" + "="*60)
print("üîç SEARCHING FOR RDB DATASET")
print("="*60)

raf_base = "/content/CV_datasets/CV datasets/DB DATASET/DATASET"

if not os.path.exists(raf_base):
    possible_paths = [
        "/content/CV_datasets/CV datasets/DB DATASET/DATASET",
        "/content/CV_datasets/CV datasets/DB DATASET",
        os.path.join(extract_path, "CV datasets/DB DATASET/DATASET"),
        os.path.join(extract_path, "CV datasets/DB DATASET"),
    ]

    for path in possible_paths:
        if os.path.exists(path):
            raf_base = path
            break

if not os.path.exists(raf_base):
    raise FileNotFoundError(f" DB not found at expected location: {raf_base}")

print(f" Found RAF-DB at: {raf_base}")

print(f"\n Contents of RAF-DB:")
for item in os.listdir(raf_base):
    item_path = os.path.join(raf_base, item)
    if os.path.isdir(item_path):
        try:
            count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])
            print(f"   {item}/ ({count} items)")
        except:
            print(f"   {item}/")
    else:
        print(f"   {item}")

# train_labels.csv
train_labels_file = os.path.join(os.path.dirname(raf_base), "train_labels.csv")
test_labels_file = os.path.join(os.path.dirname(raf_base), "test_labels.csv")

image_labels = {}

# train labels
if os.path.exists(train_labels_file):
    print(f" Reading: {train_labels_file}")
    df_train = pd.read_csv(train_labels_file)
    print(f"   Columns: {list(df_train.columns)}")
    print(f"   Shape: {df_train.shape}")
    print(f"   Sample:\n{df_train.head()}")

    for idx, row in df_train.iterrows():
        img_name = str(row.iloc[0])
        emotion_id = int(row.iloc[1])
        if emotion_id in raf_mapping:
            image_labels[img_name] = raf_mapping[emotion_id]

# test labels
if os.path.exists(test_labels_file):
    print(f"\n Reading: {test_labels_file}")
    df_test = pd.read_csv(test_labels_file)
    print(f"   Columns: {list(df_test.columns)}")
    print(f"   Shape: {df_test.shape}")
    print(f"   Sample:\n{df_test.head()}")

    for idx, row in df_test.iterrows():
        img_name = str(row.iloc[0])
        emotion_id = int(row.iloc[1])
        if emotion_id in raf_mapping:
            image_labels[img_name] = raf_mapping[emotion_id]

print(f"\n Loaded {len(image_labels)} labels total")

# ===================== COLLECT IMAGES =====================
print("\n" + "="*60)
print(" COLLECTING IMAGES")
print("="*60)

images_by_label = {}


for split in ['train', 'test']:
    split_path = os.path.join(raf_base, split)
    if os.path.exists(split_path):
        print(f"\n Processing {split}/")

        for emotion_id, label in raf_mapping.items():
            emotion_folder = os.path.join(split_path, str(emotion_id))
            if os.path.exists(emotion_folder):
                images = glob.glob(os.path.join(emotion_folder, "*.jpg")) + \
                        glob.glob(os.path.join(emotion_folder, "*.jpeg")) + \
                        glob.glob(os.path.join(emotion_folder, "*.png"))

                if images:
                    print(f"   {emotion_id}/ ({label}): {len(images)} images")
                    if label not in images_by_label:
                        images_by_label[label] = []
                    images_by_label[label].extend(images)

if not images_by_label:
    print("\n No numbered folders found. Searching all images...")

    all_images = glob.glob(os.path.join(raf_base, "**/*.jpg"), recursive=True) + \
                 glob.glob(os.path.join(raf_base, "**/*.jpeg"), recursive=True) + \
                 glob.glob(os.path.join(raf_base, "**/*.png"), recursive=True)

    print(f" Found {len(all_images)} total images")

    matched = 0
    unmatched = 0

    for img_path in all_images:
        img_name = os.path.basename(img_path)

        possible_keys = [
            img_name,
            img_name.replace('.jpg', ''),
            img_name.replace('.jpeg', ''),
            img_name.replace('.png', ''),
            os.path.splitext(img_name)[0]
        ]

        label = None
        for key in possible_keys:
            if key in image_labels:
                label = image_labels[key]
                matched += 1
                break

        if label:
            if label not in images_by_label:
                images_by_label[label] = []
            images_by_label[label].append(img_path)
        else:
            unmatched += 1

    print(f" Matched: {matched} images")
    print(f" Unmatched: {unmatched} images")

if not images_by_label:
    raise ValueError(" No images found with labels!")

print("\n" + "="*60)
print(" SUMMARY")
print("="*60)
print(f"Total labels: {len(images_by_label)}")
total_images = 0
for label, images in sorted(images_by_label.items()):
    print(f"  {label:40s}: {len(images):5d} images")
    total_images += len(images)
print(f"{'Total':42s}: {total_images:5d} images")

# ===================== PROCESS RAF-DB IMAGES =====================
print("\n" + "="*60)
print(" PROCESSING RAF-DB DATASET")
print("="*60)

all_metadata = []

for label, images in images_by_label.items():
    print(f"\n Processing {label}...")
    metadata = process_and_save(images, label, "RAF-DB", processed_path, augment=True)
    all_metadata.extend(metadata)
    clean_memory()

# ===================== SAVE METADATA =====================
save_metadata(all_metadata)

print("\n" + "="*60)
print("DB PROCESSING COMPLETE!")
print("="*60)
print(f" Total processed images: {len(all_metadata)}")
print(f"Metadata saved at: {metadata_file}")

print("\n Final Statistics:")
for label in sorted(images_by_label.keys()):
    original = sum(1 for m in all_metadata if m[2] == label and m[4] == "original")
    augmented = sum(1 for m in all_metadata if m[2] == label and m[4] == "augmented")
    total = original + augmented
    print(f"  {label:40s}: {original:4d} original + {augmented:4d} augmented = {total:4d} total")

import torch
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

labels = df['label'].values
classes = np.unique(labels)
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)
print("Class weights:", class_weights_tensor)

import os

def get_folder_size(folder):
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(folder):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if os.path.exists(fp):
                total_size += os.path.getsize(fp)
    return total_size

folder = "/content/drive/MyDrive/processed_dataset"
size_bytes = get_folder_size(folder)
size_gb = size_bytes / (1024**3)
print(f"Folder size: {size_gb:.2f} GB")

!du -sh "/content/drive/MyDrive/processed_dataset"

import os
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from tqdm import tqdm
import cv2

class DatasetVerifier:
    def __init__(self, dataset_path, metadata_path=None):
        self.dataset_path = Path(dataset_path)
        self.metadata_path = metadata_path
        self.results = {
            'corrupted_images': [],
            'wrong_size': [],
            'wrong_channels': [],
            'missing_files': [],
            'class_distribution': {},
            'total_images': 0
        }

    def verify_all_images(self):
        """Comprehensive verification of all images"""
        print("üîç Starting comprehensive Dataset scan...")

        classes = [d for d in self.dataset_path.iterdir() if d.is_dir()]

        for class_dir in tqdm(classes, desc="Checking classes"):
            class_name = class_dir.name
            images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png'))

            valid_count = 0

            for img_path in tqdm(images, desc=f"  {class_name}", leave=False):
                try:
                    # Try to open the image
                    img = Image.open(img_path)
                    img_array = np.array(img)

                    # Check size
                    if img_array.shape[:2] != (224, 224):
                        self.results['wrong_size'].append(str(img_path))

                    # Check channels (RGB)
                    if len(img_array.shape) != 3 or img_array.shape[2] != 3:
                        self.results['wrong_channels'].append(str(img_path))

                    # Check values
                    if img_array.max() == 0 or img_array.min() == img_array.max():
                        self.results['corrupted_images'].append(str(img_path))

                    valid_count += 1

                except Exception as e:
                    self.results['corrupted_images'].append({
                        'path': str(img_path),
                        'error': str(e)
                    })

            self.results['class_distribution'][class_name] = valid_count
            self.results['total_images'] += valid_count

        return self.results

    def analyze_class_balance(self):
        """Analyze class balance"""
        dist = self.results['class_distribution']

        df = pd.DataFrame(list(dist.items()), columns=['Class', 'Count'])
        df = df.sort_values('Count', ascending=False)

        max_count = df['Count'].max()
        min_count = df['Count'].min()
        imbalance_ratio = max_count / min_count

        print("\n" + "="*60)
        print(" Class Distribution Analysis")
        print("="*60)
        print(df.to_string(index=False))
        print("\n" + "-"*60)
        print(f"Total images: {self.results['total_images']:,}")
        print(f"Largest class: {df.iloc[0]['Class']} ({df.iloc[0]['Count']:,} images)")
        print(f"Smallest class: {df.iloc[-1]['Class']} ({df.iloc[-1]['Count']:,} images)")
        print(f"Imbalance ratio: {imbalance_ratio:.2f}:1")



    def plot_distribution(self, save_path='class_distribution.png'):
        """Plot class distribution"""
        dist = self.results['class_distribution']

        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        # Bar Chart
        classes = list(dist.keys())
        counts = list(dist.values())

        colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))
        axes[0].bar(classes, counts, color=colors, edgecolor='black', linewidth=1.5)
        axes[0].set_xlabel('Classes', fontsize=12, fontweight='bold')
        axes[0].set_ylabel('Number of Images', fontsize=12, fontweight='bold')
        axes[0].set_title('Class Distribution - Bar Chart', fontsize=14, fontweight='bold')
        axes[0].tick_params(axis='x', rotation=45)
        axes[0].grid(axis='y', alpha=0.3)

        # Add numbers on top of bars
        for i, (c, count) in enumerate(zip(classes, counts)):
            axes[0].text(i, count, f'{count:,}', ha='center', va='bottom', fontweight='bold')

        # Pie Chart
        axes[1].pie(counts, labels=classes, autopct='%1.1f%%', startangle=90, colors=colors)
        axes[1].set_title('Class Proportions - Pie Chart', fontsize=14, fontweight='bold')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\n Chart saved: {save_path}")
        plt.close()

    def generate_report(self, output_path='dataset_verification_report.txt'):
        """Generate comprehensive report"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write(" Comprehensive Dataset Verification Report\n")
            f.write("="*70 + "\n\n")

            # General summary
            f.write(" General Summary\n")
            f.write("-"*70 + "\n")
            f.write(f"Total images: {self.results['total_images']:,}\n")
            f.write(f"Number of classes: {len(self.results['class_distribution'])}\n\n")

            # Class distribution
            f.write(" Class Distribution\n")
            f.write("-"*70 + "\n")
            for cls, count in sorted(self.results['class_distribution'].items(),
                                   key=lambda x: x[1], reverse=True):
                percentage = (count / self.results['total_images']) * 100
                f.write(f"{cls:35s} {count:8,} ({percentage:5.2f}%)\n")

            # Detected issues
            f.write("\n Detected Issues\n")
            f.write("-"*70 + "\n")
            f.write(f"Corrupted images: {len(self.results['corrupted_images'])}\n")
            f.write(f"Wrong size images: {len(self.results['wrong_size'])}\n")
            f.write(f"Wrong channels images: {len(self.results['wrong_channels'])}\n")

            if self.results['corrupted_images']:
                f.write("\n  Corrupted images list:\n")
                for img in self.results['corrupted_images']:
                    f.write(f"  - {img}\n")

            if not any([self.results['corrupted_images'],
                       self.results['wrong_size'],
                       self.results['wrong_channels']]):
                f.write(" No issues detected!\n")

        print(f"\n Report saved: {output_path}")

    def clean_corrupted_images(self):
        """Delete corrupted images"""
        if not self.results['corrupted_images']:
            print(" No corrupted images to delete")
            return

        print(f"\n  Deleting {len(self.results['corrupted_images'])} corrupted images...")

        for img_info in self.results['corrupted_images']:
            try:
                img_path = img_info if isinstance(img_info, str) else img_info['path']
                os.remove(img_path)
                print(f"  ‚úì Deleted: {img_path}")
            except Exception as e:
                print(f"  ‚úó Failed to delete: {img_path} - {e}")