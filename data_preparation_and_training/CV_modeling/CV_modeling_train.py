# -*- coding: utf-8 -*-
"""final_model_NOV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fXJDjFFpyjuOHt1fML8YJc-GRZHUm7z0
"""

from google.colab import drive
import os

drive.mount('/content/drive')

drive_data_path = '/content/drive/MyDrive/FER-2013/processed'
local_data_path = '/content/processed'
os.makedirs(local_data_path, exist_ok=True)

!cp -r {drive_data_path}/* {local_data_path}/

train_path = os.path.join(local_data_path, 'train')
val_path   = os.path.join(local_data_path, 'validation')

# Data Augmentation
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

IMG_SIZE = (224, 224)
BATCH_SIZE = 64

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    zoom_range=0.2,
    brightness_range=[0.8, 1.2],
    channel_shift_range=0.1,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(
    rescale=1./255
)

train_gen = train_datagen.flow_from_directory(
    train_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=True,
    seed=42,
    color_mode='rgb',
    interpolation='bilinear'
)

val_gen = val_datagen.flow_from_directory(
    val_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False,
    color_mode='rgb',
    interpolation='bilinear'
)

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models, regularizers
import tensorflow as tf

def create_transfer_learning_model(num_classes=6):
    base_model = EfficientNetB0(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3),
        pooling='avg'
    )
    base_model.trainable = False

    inputs = tf.keras.Input(shape=(224, 224, 3))
    x = tf.keras.applications.efficientnet.preprocess_input(inputs)
    x = base_model(x, training=False)

    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    outputs = layers.Dense(num_classes, activation='softmax', name='output')(x)

    model = models.Model(inputs=inputs, outputs=outputs, name='EfficientNetB0_FER2013')
    return model, base_model

from tensorflow.keras.optimizers import Adam

model, base_model = create_transfer_learning_model(num_classes=6)

Feature
drive_checkpoint = '/content/drive/MyDrive/hope/EfficientNetB0_best_finetuned.weights.h5'
model.load_weights(drive_checkpoint)
print("Fine-Tuning")

for layer in base_model.layers[:-170]:
    layer.trainable = False
for layer in base_model.layers[-170:]:
    layer.trainable = True

print(" The last 170 layers have been unlocked for fine-tuning")

ft_lr = 1e-5
model.compile(
    optimizer=Adam(learning_rate=ft_lr),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

from tensorflow.keras.optimizers import Adam

model, base_model = create_transfer_learning_model(num_classes=6)

drive_checkpoint = '/content/drive/MyDrive/hope/EfficientNetB0_best_finetuned.weights.h5'
model.load_weights(drive_checkpoint)
print(" The best fine-tuning weights have been loaded")

model.save('/content/drive/MyDrive/hope/EfficientNetB0_finetuned_full_model_new.keras')
print("The full model has been successfully saved in the modern KERAS format!")
!ls -lh /content/drive/MyDrive/hope/EfficientNetB0_finetuned_full_model_new.keras

_

ft_local_checkpoint = '/content/EfficientNetB0_best_finetuned.weights.h5'
ft_drive_checkpoint = '/content/drive/MyDrive/hope/EfficientNetB0_best_finetuned.weights.h5'


from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau


callbacks = [
    EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=1),
    ModelCheckpoint(ft_local_checkpoint, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1),
    ModelCheckpoint(ft_drive_checkpoint, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)
]

class_weights = {
    0: 0.861,
    1: 0.879,
    2: 0.848,
    3: 0.872,
    4: 1.609,
    5: 1.328
}

loaded_ft = True

if loaded_ft:
print(" Fine-tuning will resume from the last saved state...")
else:
print(" Fine-tuning will start from the current model after feature extraction...")

history_ft = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=40,
    class_weight=class_weights,
    callbacks=callbacks
)

import os
os.makedirs('/content/drive/MyDrive/hope', exist_ok=True)
!cp {ft_local_checkpoint} {ft_drive_checkpoint}
print(f" The best fine-tuned model has been copied to Drive: {ft_drive_checkpoint}")

# Save the full model after fine-tuning in the modern KERAS format
model.save('/content/drive/MyDrive/hope/EfficientNetB0_finetuned_full_model.h5')